{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1af9ec36-9f84-4e62-96fe-bf86a6e844c9",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "---\n",
    "* 15min: Explanation of phenotype variants, genotype variants, and their associations\n",
    "* 30 min: General overview of typical GWAS pipeineline\n",
    "  1. Data wrangling- QC\n",
    "  2. Data Analysis/Modeling - chi squared, Odds Ratio, Linear Regression (expression data)\n",
    "  3. Interpretation and visualization of p-values\n",
    "     * Manhattan plots (contrasts the $−log_{10}(p-value)$ of each SNP against its genomic location, with the $\\alpha=5.0×10^{−8}$ genome-wide significance line) \n",
    "     * QQplots (the quantiles of the observed p-values against those of Unif(0,1), on the −log_10 scale.)\n",
    "     * $\\alpha=5.0×10^{−8}$ is typical for GWAS (compared to 0.05)\n",
    "* 45 min: _TAS2R38_ GWAS activity\n",
    "* Basic associations (our Model & Analysis): Odds Ratio, chi-squared tests, case-control\n",
    "\n",
    "## References\n",
    "* [General GWAS pipeline](https://pmc.ncbi.nlm.nih.gov/articles/PMC10940486)\n",
    "* [P-values for GWAS](https://onlinelibrary.wiley.com/doi/10.1002/gepi.20297)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212d3dbc-8ca5-4928-8d72-6039464cd587",
   "metadata": {},
   "source": [
    "# Overview of GWAS\n",
    "---\n",
    "## Genome Wide Association Studies illustrate all aspects of Data Science:\n",
    "  \n",
    "1. complex, large-scale and high-dimensionsal databases\n",
    "2. practically important/relevant\n",
    "3. data wrangling is crucial!\n",
    "4. demonstration of a variety of statistical techniques (Odds Ratio, chi-squared test or even linear regression, for eQTL)\n",
    "5. understanding the limits of your model (one example: association by linkage disequilibrium, NOT causality)\n",
    "6. ethics of sensitive information \n",
    "    (tangent: [a short course on eugenics](https://qubeshub.org/community/groups/coursesource/publications?id=5101&tab_active=about&v=1))\n",
    "\n",
    "## Genetic Variants\n",
    "The genetic variants that drive \"Mendelian traits\" are often straight forward to categorize. \n",
    "* Think: Punnett Squares\n",
    "* Examples: earwax type, cystic fibrosis, sickle cell anemia\n",
    "\n",
    "However, **traits that result from the contribution of many variants, each of small effect, need to be resolved with statistics and big data.**\n",
    "* Examples: height, Coronary Artery Disease, Type II diabetes\n",
    "\n",
    "## What __are__ Genome-Wide Association Studies?\n",
    "GWAS answer questions about genetic variants that are associated with complex traits:\n",
    "\n",
    "> ***Is a particular genomic variant (i.e. a Single Polymorphic Trait - \"SNP\") associated with a particular phenotype (i.e. condition or a disease)?***\n",
    "\n",
    "<br>\n",
    "\n",
    "**Example Conditions:** continuous blood pressure, binary trait of high/low blood pressure\n",
    "\n",
    "**Variant:** Usually bi-allelic SNP\n",
    "\n",
    "> **Important note: The SNP may be responsible for the phenotypic difference, but USUALLY we assume that the SNP is simply located close to the causal variant and that it is close enough to NOT have been broken up through crossing over events yet.** It is a proxy which makes it vulnerable to violations of assumptions (Wrangling is very important). \n",
    "\n",
    "<br>\n",
    "\n",
    "GWAS are also used in prioritizing variants in Pharmacogenomics. The article [Genome Wide Association Studies in Pharmacogenomics](https://ascpt.onlinelibrary.wiley.com/doi/10.1002/cpt.2349) has some illuminative figures.\n",
    "\n",
    "### Domain Knowledge Resources:\n",
    "1. \"Useful Genetics\" - Rosie Redfield\n",
    "    * [5D- Genome-wide association studies, part 1](https://www.youtube.com/watch?v=bNpYzOr7I94)\n",
    "    * [5D- Genome-wide association studies, part 2](https://www.youtube.com/watch?v=OASrRPcdM_0)\n",
    "2. [(Slightly more in depth explanation)](https://www.youtube.com/watch?v=sOP8WacfBM8&t=217s)\n",
    "3. [Example GWAS with Plink (software, independent of Python, that is focused on genomic analysis, such as GWAS)](https://www.youtube.com/watch?v=7QMSZx3io-Q&t=1s)\n",
    "4. [Quick two page overview of GWAS](https://www.cell.com/current-biology/pdf/S0960-9822(13)00075-4.pdf)\n",
    "5. [Even shorter GWAS Overview](https://www.yourgenome.org/theme/genome-wide-association-studies/)\n",
    "6. [NHGRI GWAS Fact Sheet](https://www.genome.gov/about-genomics/fact-sheets/Genome-Wide-Association-Studies-Fact-Sheet)\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f777dfc5-0757-42e4-9721-b049068068c0",
   "metadata": {},
   "source": [
    "## Overall Strategy of GWAS\n",
    "1. Find many thousands of people who differ in the trait of interest\n",
    "2. Use some kind of DNA chip (or other) to genotype their alleles at each of $10^7$ locations\n",
    "3. Identify SNPs where the two phenotypic categories (very tall versus very short) have different allele frequencies\n",
    "\n",
    "The images below help illustrate this process.\n",
    "* First cartoon is a screenshot from Roside Redfield's Useful Genetics linked in the cell above\n",
    "* Second is from Your Genome (the \"even shorter\" link in cell above)\n",
    "* Third cartoon is from the NHGRI Fact Sheet\n",
    "\n",
    "![](https://raw.githubusercontent.com/awnorowski/BDSiC_2025/refs/heads/main/images/GWASStrategyRedfield.png)\n",
    "\n",
    "![](https://www.yourgenome.org/wp-content/uploads/2023/11/161-2015-03-12_genome-association-studies-yourgenome_CS_ES-768x461.png)\n",
    "\n",
    "![](https://www.genome.gov/sites/default/files/inline-images/GWAS_Fact-sheet2020.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4424b6-8904-464d-be47-1d8af5b751b1",
   "metadata": {},
   "source": [
    "# DS Pipeline for GWAS\n",
    "---\n",
    "1. __Data__\n",
    "   \n",
    "   1. Datasets include:\n",
    "       1. Genomic variation: microarrays (common variants), WGS (can include rare variants)\n",
    "       2. Phenotypic variation\n",
    "   2. Data sources include:\n",
    "       * Biobanks, cohorts with disease-focused or population-based recruitment, direct consumer studies\n",
    "        * Example: GWAS Catalogue\n",
    "           * We will investigate this in 5c, and if you are interested, you can check out the summary statistics page which is useful and will give you tsv files that contain Odds Ratios and p-values) <br><br>\n",
    "    \n",
    "2. __Quality Control: Wrangling the Data__\n",
    "\n",
    "    Each particular dataset and question you investigate will require a pipeline to address unique challenges that arise from unique aspects of the data/question. Genomic databases are a great example of a specialized pipeline to reduce the bias of non-independence on top of the more typical missing or inaccurate data grooming: \n",
    "\n",
    "    1. removing rare or monomorphic variants - minor allele frequency\n",
    "    2. filtering missing SNPs \n",
    "    3. identifying and removing genotyping errors\n",
    "        * heterozygosity\n",
    "        * sex discrepancy\n",
    "        * removing variants not in Hardy-Weinberg equilibrium (can indicate: genotyping errors, batch effects, population stratification)\n",
    "        * Use PCA - a straightforward way to identify population stratification or batch effects\n",
    "    4. accounting for ancestry and relatedness - another way population stratification pops up! Cases and controls should be matched by ancestry to avoid confounding and, therefore, false positives. \n",
    "    5. account for false positives (you are testing millions of hypotheses simultaneously)  <br><br>\n",
    "\n",
    "    For your own information (we will not use this in this overview): The program Plink is standard for analysis GWAS and it requires files of specific formats: bed, fam, bim files all contain slightly different information, including genomic variants, and family relationships. **Plink makes the quality control step easier (a huge challenge) with built-in functions for the above.** There are python packages that wrap around Plink and multivariate analysis tools to create a seamless pipeline. \n",
    "\n",
    "3. __Analysis:__\n",
    "   Common models/analysis for GWAS:\n",
    "   \n",
    "    1. Odds ratio\n",
    "    2. Chi-squared tests\n",
    "    3. Regression:\n",
    "        * linear or logistic multivariate regression (eQTL)\n",
    "        * generalized linear mixed-effect models\n",
    "        * account for covariates: sex, age, ancestry <br><br>\n",
    "\n",
    "5. __Visualization__ \n",
    "    * p-value Manhattan plots\n",
    "    * QQplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad0ce67-9f62-4a88-83b6-37fc8ffe423f",
   "metadata": {},
   "source": [
    "# _TAS2R38_ GWAS Activity\n",
    "---\n",
    "This activity was adapted from the The Jackson Laboratory's [Teaching the Genome Generation](https://www.jax.org/education-and-learning/high-school-students-and-undergraduates/teaching-the-genome-generation) curriculum.\n",
    "\n",
    "## The _TAS2R38_ Gene Activity Overview\n",
    "* Work through the calculations involved in the association between bitter taste perception and variants in the _TAS2R38_ gene.\n",
    "* Taste 2 Receptor Member 38 produces a protein that is responsible for bitter taste (especially on the tongue) \n",
    "* Categorical phenotypes: Non-taster, Taster, Super Tasteres\n",
    "* There are three SNPs that are common variants in the _TAS2R38_ gene: SNP1, SNP2, SNP3\n",
    "\n",
    "### Group Activity\n",
    "\n",
    "1. Split into groups of 3 (~5 groups) and you will be given the information for one of the 3 SNPs.\n",
    "\n",
    "**SNP1** <br>\n",
    "![](https://raw.githubusercontent.com/awnorowski/BDSiC_2025/refs/heads/main/images/GWAS_SNP1.png) <br><br>\n",
    "**SNP2** <br>\n",
    "![](https://raw.githubusercontent.com/awnorowski/BDSiC_2025/refs/heads/main/images/GWAS_SNP2.png) <br><br>\n",
    "**SNP3** <br>\n",
    "![](https://raw.githubusercontent.com/awnorowski/BDSiC_2025/refs/heads/main/images/GWAS_SNP3.png) <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d612d2c6-0bca-43ee-8cff-a2160381b991",
   "metadata": {},
   "source": [
    "2. (10 minutes) Count the number of allele variants associated with each of the three phenotypes for your particular SNP.\n",
    "   \n",
    "3. (5 minutes) We will then come back together as a class and discuss which variants - if any - of SNP1, SNP2, and SNP3 are associated with the three classes of tasting phenotype.\n",
    "\n",
    "4. (10 minutes) Discussion of how we would *quantify* evidence for how variants might be associated with a phenotype?\n",
    "\n",
    "5. (20 minutes) $\\chi^2$ test and Odds Ratio\n",
    "    1. First, focus on your SNP (1, 2, or 3) and calculate **expected count** of each of the two variants _if the variant was not associated with a particular phenotype_.\n",
    "        * Note: this is the NULL hypothesis of _NO ASSOCIATION_ which is what we are testing.\n",
    "        * **Equation:** Expected count = Allele freq of allele in total pop * total Allele count for specific phenotype\n",
    "    2. Then, calculate **$\\chi^2$**\n",
    "        * **General Equation:**  $\\chi^2 = \\sum_{i=1}^{k}\\frac{(x_i-m_i)^2}{m_i}$\n",
    "        * There are two alleles per SNP so, the equation will be: $\\chi^2 = \\frac{(O_1-E_1)^2}{E_1} + \\frac{(O_2-E_2)^2}{E_2}$\n",
    "            * For this example, you are investigating one SNP so it is reasonable to do it by hand. However, GWAS looks at 10,000,000s of SNPs simultaneously, a scale that makes computation more challenging *and* introduces some novel problems (multiple testing means lots of false positives!)\n",
    "            * There are built in functions to do these calculations for you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6ec2bb-281f-4fad-b02c-8e787226eafd",
   "metadata": {},
   "source": [
    "# Data Analysis Practice\n",
    "\n",
    "---\n",
    "\n",
    "## We will do the following: \n",
    "1. Data -- We will look at some in module 5c when we build Manhattan plots\n",
    "2. Wrangling/Grooming - Module 6B when we investigate general quality control tactics\n",
    "3. Analysis - we will work through pen-and-paper example for $\\chi^2$ test and for Odds ratio. In module 5B will cover regression.\n",
    "4. Visualization - Module 5C Manhattan plots (and QQ plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586a5c00-f0f5-4542-a7a9-7cbd8d916b46",
   "metadata": {},
   "source": [
    "## Analysis/Model - $\\chi^2$ test pipeline: \n",
    "1. State your null hypothesis\n",
    "2. Pick your test; make sure assumptions are valid for your data; conduct your test\n",
    "3. evidence level of your test (what is $\\alpha$? What is p-value, if program calculates it for you)\n",
    "4. Can you Reject or Fail-to-Reject your null hypothesis?\n",
    "\n",
    "[Here is a review of $\\chi^2$](https://www.geeksforgeeks.org/python-pearsons-chi-square-test/)\n",
    "\n",
    "__After using $\\chi^2$ we are only able to reject or Fail to reject the null hypothesis, but that isn't very satisfying, is it?__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbdfa8a-8f07-4394-ade7-cee775302253",
   "metadata": {},
   "source": [
    "## Analysis/Model - Odds Ratio\n",
    "If we have two categories that each have two categories, we can use the Odds Ratio to *quantify* the strength of an association instead of just getting a R/FTR, we can now also state magnitude of effect.  \n",
    "\n",
    "_ODDS RATIO_: Now we want to quantify the _magnitude of the association between two categorical variables that each have two categories_\n",
    "\n",
    "- often used in __case-control groups__\n",
    "- usually testing $H_{0}$ = 1 which means that the odds of having the disease given you have the AT variant is 1 (no inflated odds)\n",
    "\n",
    "Here is the [link to wikipedia](https://en.wikipedia.org/wiki/Odds_ratio) that explains the basics of OR. \n",
    "\n",
    "Here is an [example](https://rowannicholls.github.io/python/statistics/risk/odds_ratio.html) of how you would take the wikipedia example, diseases in people exposed to a radiation leak and in people who were not exposed to a radiation leak. \n",
    "\n",
    ">\"Suppose a radiation leak in a village of 1,000 people increased the incidence of a rare disease. The total number of people exposed to the radiation was 400, out of which 20 developed the disease and 380 stayed healthy. The total number of people not exposed was 600, out of which 6 developed the disease and 594 stayed healthy.\"\n",
    "\n",
    "We want to input a table that looks like this: \n",
    "\n",
    "|Exposed\\Diseased| True | False|\n",
    "|---| ---| ---|\n",
    "|True|20|380|\n",
    "|False|6|594|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70d844e5-5425-48e7-91ae-842773fe63bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdd2066f-4bb4-41b2-9c03-b16ac6207a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diseased</th>\n",
       "      <th>Not Diseased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Exposed</th>\n",
       "      <td>20</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Not Exposed</th>\n",
       "      <td>6</td>\n",
       "      <td>594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Diseased  Not Diseased\n",
       "Exposed            20           380\n",
       "Not Exposed         6           594"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There is another example of the same thing, here: https://rowannicholls.github.io/python/statistics/risk/odds_ratio.html\n",
    "# The example from the above website shows you a more complicated way to create this table\n",
    "# that gives you all 1000 data points but shows you some methods that might be useful. \n",
    "# We will use simpler code for the following example: \n",
    "\n",
    "# Step 1: Define the 2x2 contingency table\n",
    "table_wiki = [[20, 380],\n",
    "         [6, 594]]\n",
    "\n",
    "# Create a DataFrame with headings and row names\n",
    "df_wiki = pd.DataFrame(table_wiki, \n",
    "                  columns=['Diseased', 'Not Diseased'], \n",
    "                  index=['Exposed', 'Not Exposed'])\n",
    "\n",
    "df_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "123839be-ae05-4b60-9b0e-9ed91e0faf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exposed        400\n",
      "Not Exposed    600\n",
      "dtype: int64\n",
      "********************\n",
      "risk\n",
      "             Diseased  Not Diseased\n",
      "Exposed          0.05          0.95\n",
      "Not Exposed      0.01          0.99\n",
      "********************\n",
      "~~~~~~~~~~~~~~~~~~\n",
      "Risk in Exposed group: 0.0500\n",
      "Risk in Unexposed group: 0.0100\n",
      "Relative Risk (RR): 5.000\n"
     ]
    }
   ],
   "source": [
    "# Risk: The risk of developing the disease given exposure, and of developing the disease given non-exposure, \n",
    "# is equal to the number of people who became diseased (20 and 6) divided by the total number of people who \n",
    "# were exposed or not exposed (400 and 600), respectively. In other words, we need to sum the values in the \n",
    "# contingency table’s rows and divide the contingency table by those values.\n",
    "\n",
    "# Step 2: Extract the individual four cell values\n",
    "a = df_wiki.loc['Exposed', 'Diseased']\n",
    "b = df_wiki.loc['Exposed', 'Not Diseased']\n",
    "c = df_wiki.loc['Not Exposed', 'Diseased']\n",
    "d = df_wiki.loc['Not Exposed', 'Not Diseased']\n",
    "\n",
    "\n",
    "# Step 3A: Calculate individual Risk \n",
    "# Get the total of each row, axis=1\n",
    "totals = df_wiki.sum(axis=1)\n",
    "print(totals)\n",
    "print(\"********************\")\n",
    "# Divide the values in the dataframe by the total of their row\n",
    "# risk is 20/(380+20)\n",
    "risk = df_wiki.div(totals, axis=0)\n",
    "print(\"risk\")\n",
    "print(risk)\n",
    "print(\"********************\")\n",
    "#The relative risk of developing the disease given expose vs non-exposure is simply one risk value divided by another: 0.05/0.01\n",
    "\n",
    "# Step 3b: Calculate Relative Risk\n",
    "risk_exposed = a / (a + b)\n",
    "risk_unexposed = c / (c + d)\n",
    "relative_risk = risk_exposed / risk_unexposed\n",
    "print(\"~~~~~~~~~~~~~~~~~~\")\n",
    "print(f\"Risk in Exposed group: {risk_exposed:.4f}\")\n",
    "print(f\"Risk in Unexposed group: {risk_unexposed:.4f}\")\n",
    "print(f\"Relative Risk (RR): {relative_risk:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d380c724-bf62-4436-a796-aa45b26bfa14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odds for exposed 0.05\n",
      "odds for exposed 0.01\n",
      "Odds Ratio:  5.2\n",
      "SignificanceResult(statistic=5.2105263157894735, pvalue=0.000145624155415758)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Odds and Odds Ratio\n",
    "# odds: The odds of getting the disease if exposed is the ratio of the number of people that became diseased to the number\n",
    "# that did not - ie 20 divided by 380 - and similar for those who were not exposed:\n",
    "odds_exposed = a/b\n",
    "print(\"odds for exposed\",round(odds_exposed,2))\n",
    "odds_unexposed = c/d\n",
    "print(\"odds for exposed\",round(odds_unexposed,2))\n",
    "# Odds Ratio\n",
    "odds_ratio = odds_exposed / odds_unexposed\n",
    "print(\"Odds Ratio: \", round(odds_ratio,1))\n",
    "\n",
    "# Step 5: if you want to get a p-value for this, use a fisher's exact test from scipy.stats\n",
    "print(scipy.stats.fisher_exact(table_wiki))\n",
    "# you can see the pvalue is 0.000146 and the odds ratio is the same, which is comforting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33f9eeea-6e6b-4b87-a502-90da82b18f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case control contingency table\n",
      "diseased  False  True \n",
      "exposed               \n",
      "False        16      6\n",
      "True         10     20\n",
      "Case-Control Odds Ratio: \n",
      "Odds Ratio:  5.3\n"
     ]
    }
   ],
   "source": [
    "# CASE-CONTROL EXAMPLE:\n",
    "# The Wikipedia page goes on to give a second example wherein the data from all 26 diseased villagers\n",
    "# is included but only that from 26 of the healthy villagers is available (which is a more realistic scenario):\n",
    "# same data wrangling as in the above example, but now we CAN'T compute relative risk because we don't know the true\n",
    "# exposed/unexposed in the population. \n",
    "\n",
    "# Create a data frame from a dictionary\n",
    "dct = {\n",
    "    'exposed': [True] * 30 + [False] * 22,\n",
    "    'diseased': [True] * 20 + [False] * 10 + [True] * 6 + [False] * 16,\n",
    "}\n",
    "df = pd.DataFrame(dct)\n",
    "contingency_table = pd.crosstab(df['exposed'], df['diseased'])\n",
    "# Optional: we could have done this via this method: reverse index and columns for presentation (matches Wikipedia formatting)\n",
    "# contingency = contingency.iloc[::-1, ::-1]\n",
    "\n",
    "print(\"Case control contingency table\")\n",
    "print(contingency_table)\n",
    "# The relative risk cannot be calculated because we don’t have data from the entire population,\n",
    "# but we can get the odds ratio by follow the same steps as above:\n",
    "odds = contingency_table[True] / contingency_table[False]\n",
    "odds_ratio = odds[True] / odds[False]\n",
    "print(\"Case-Control Odds Ratio: \")\n",
    "print(\"Odds Ratio: \", round(odds_ratio,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5183372-4b7a-4d28-9e04-46f3aee2df14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BDSC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
